{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hide_input": true,
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mp\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "from IPython.display import Image, HTML\n",
    "\n",
    "import laUtilities as ut\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most of the machine learning we have studied this semester is based on the idea that we have a model is that _parameterized_, and our goal is to find good settings for the parameters.\n",
    "\n",
    "We have seen example after example of this problem.\n",
    "\n",
    "* In $k$-means, our goal was to find $k$ cluster centroids, so that the $k$-means objective was minimized.\n",
    "\n",
    "* In linear regression, our goal was to find a parameter vector $\\beta$ so that sum of squared error $\\Vert \\mathbf{y} - \\hat{\\mathbf{y}}\\Vert$ was minimized.\n",
    "\n",
    "* In the support vector machine, our goal was to find a parameter vector $\\theta$ so that classification error was minimized.\n",
    "\n",
    "And on and on ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time now to talk about how, in general, one can find \"good settings\" for the parameters in problems like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What allows us to unify our approach to many such problems is the following:\n",
    "\n",
    "First, we start by defining an error function, generally called a __loss__ function, to describe how well our method is doing.\n",
    "\n",
    "And second, we choose loss functions that are __differentiable__ with respect to the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two requirements mean that we can think of the parameter tuning problem using these sorts of surfaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "    \n",
    "<img src=\"figs/L23-convex_cost_function.jpeg\" width=\"75%\">\n",
    "    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that the $x$ and $y$ axes in these pictures represent parameter settings.   That is, we have two parameters to set, corresponding to the values of $x$ and $y$.\n",
    "\n",
    "For each $(x, y)$ setting, the $z$-axis shows the value of the loss function. \n",
    "\n",
    "What we want to do is find the minimum of a surface, corresponding to the parameter settings that minimize loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference between the two kinds of surfaces.    \n",
    "\n",
    "The surface on the left corresponds to a __strictly convex__ loss function.   If we find a local minimum of this function, it is a global minimum.\n",
    "\n",
    "The surface on the right corresponds to a __non-convex__ loss function.   There are local minima that are not globally minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both kinds of loss functions arise in machine learning.\n",
    "\n",
    "For example, convex loss functions arise in\n",
    "* Linear regression\n",
    "* Logistic regression\n",
    "\n",
    "While non-convex loss functions arise in \n",
    "* $k$-means\n",
    "* Gaussian Mixture Modeling\n",
    "* and many other settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Intuitively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition of gradient descent is the following.   \n",
    "\n",
    "Imagine you are lost in the mountains, and it is foggy out.  You want to find a valley.  But since it is foggy, you can only see the local area around you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image credit http://nederlandliving.com/?p=1931 -->\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"figs/L23-fog-in-the-mountains.jpeg\" width=\"60%\">\n",
    "    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The natural thing to do is: \n",
    "1. Look around you 360 degrees.  \n",
    "2. Observe in which direction the ground is sloping downward most steeply.  \n",
    "3. Take a few steps in that direction.  \n",
    "4. Repeat the process <BR> ... until the ground seems to be level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key to this intuitive idea is formalizing the idea of \"direction of steepest descent.\"\n",
    "\n",
    "This is where the differentiability of the loss function comes into play.\n",
    "\n",
    "As long as the loss function is differentiable, we can define the direction of steepest descent (really, ascent).\n",
    "\n",
    "That direction is called the __gradient.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is a generalization of the slope of a line.\n",
    "\n",
    "Let's say we have a loss function $\\mathcal{L}(\\mathbf{w})$.   \n",
    "\n",
    "The components of $\\mathbf{w}\\in\\mathbb{R}^n$ are the parameters we want to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the gradient, we take the partial derivative of our loss function with respect to each parameter:\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial w_i} $$\n",
    "\n",
    "and collect all the partial derivatives into a vector of the same shape as $\\mathbf{w}$:\n",
    "\n",
    "$$ \\nabla_\\mathbf{w}\\mathcal{L} = \\begin{bmatrix}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_1}\\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_2}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_n}\n",
    "   \\end{bmatrix}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you see the notation  $\\nabla_\\mathbf{w}\\mathcal{L},$ think of it as $ \\frac{d\\mathcal{L}}{d\\mathbf{w}} $, except that $\\mathbf{w}$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that if we are going to take a small step of unit length, then the gradient is the direction that maximizes the change in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Image credit https://www.analyticsvidhya.com/blog/2020/10/how-does-the-gradient-descent-algorithm-work-in-machine-learning/ -->\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"figs/L23-gradient-of-convex.png\" width=\"60%\">\n",
    "    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above figure, in general the gradient varies depending on where you are in the parameter space.\n",
    "\n",
    "So we write:\n",
    "\n",
    "$$ \\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}) = \\begin{bmatrix}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_1}(\\mathbf{w})\\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_2}(\\mathbf{w})\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial w_n}(\\mathbf{w})\n",
    "   \\end{bmatrix}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we seek to improve our parameter estimates $\\mathbf{w}$, we will take a step in the opposite direction of the gradient.\n",
    "\n",
    "\"opposite direction\" because the gradient specifies the direction of maximum increase, and we want to decrease, the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long should our step be?  \n",
    "\n",
    "For step size, will use a scalar value $\\eta$ called the __learning rate.__\n",
    "\n",
    "The learning rate is a hyperparameter that needs to be tuned for a given problem, or even can be modified adaptively as the algorithm progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can write the __gradient descent__ algorithm formally:\n",
    "\n",
    "1. Start with an initial parameter estimate $\\mathbf{w}^0$.\n",
    "2. Update: $\\mathbf{w}^{n+1} = \\mathbf{w}^n - \\eta \\nabla_\\mathbf{w}\\mathcal{L}(\\mathbf{w}^n)$\n",
    "3. If not converged, go to step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we know if we are \"converged\"?  \n",
    "\n",
    "Typically we stop the iteration if the loss has not improved by a fixed amount for a pre-decided number, say 10 or 20, iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an animation of the sequence of points on the loss surface explored by gradient descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- https://blog.paperspace.com/intro-to-optimization-in-deep-learning-gradient-descent/ -->\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"figs/L23-gradient-animation.gif\" width=\"60%\">\n",
    "    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the improvement in loss decreases over time.  Initially the gradient is steep and loss improves fast, while later on the gradient is shallow and loss doesn't improve much per step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- https://www.cs.umd.edu/~tomg/projects/landscapes/ -->\n",
    "\n",
    "<center>\n",
    "    \n",
    "<img src=\"figs/L23-complex-landscape.png\" width=\"40%\">\n",
    "    \n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example. the likelihood function for logistic regression is:\n",
    "\n",
    "define $\\sigma(x) = \\text{logit}^{-1}(x) = \\frac{1}{1+e^{-x}} $\n",
    "\n",
    "$$L(\\alpha, \\beta \\mid x_i, y_i) = \\sigma(\\alpha + \\beta x_i)^{y_i} (1-\\sigma(\\alpha + \\beta x_i))^{1-y_i}$$\n",
    "\n",
    "or more generally:\n",
    "\n",
    "\n",
    "$$L(\\alpha, \\beta) = \\sum_i \\sigma(\\alpha + \\beta x_i)^{y_i} (1-\\sigma(\\alpha + \\beta x_i))^{1-y_i}$$\n",
    "\n",
    "We'd like to maximize the likelihood.  We'll take the log of it for convenience.\n",
    "\n",
    "$$\\log L(\\alpha, \\beta) = \\sum_i \\log(\\sigma(\\alpha + \\beta x_i)^{y_i} (1-\\sigma(\\alpha + \\beta x_i))^{1-y_i})$$\n",
    "\n",
    "$$= \\sum_i y_i \\log(\\sigma(\\alpha + \\beta x_i))+ (1-y_i) \\log(1-\\sigma(\\alpha + \\beta x_i))$$\n",
    "\n",
    "To create a loss function that we can minimize, we negate the log likelihood.   Negative log likelihood is called __cross entropy__ loss.\n",
    "\n",
    "$$\\mathcal{L} = - \\sum_i y_i \\log(\\sigma(\\alpha + \\beta x_i))+ (1-y_i) \\log(1-\\sigma(\\alpha + \\beta x_i))$$"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "rise": {
   "scroll": true,
   "theme": "beige",
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
