
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Distances and Timeseries &#8212; Computational Tools for Data Science</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Clustering I: \(k\)-means" href="06-Clustering-I-kmeans.html" />
    <link rel="prev" title="Probability and Statistics Refresher" href="03-Probability-and-Statistics-Refresher.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/L09-MultivariateNormal.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Computational Tools for Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="landing-page.html">
   Preface
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Preliminaries
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01-Intro-to-Python.html">
   Introduction to Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02A-Git-Jupyter.html">
   Essential Tools: Git and Jupyter
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02B-Pandas.html">
   Essential Tools: Pandas
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04-Linear-Algebra-Refresher.html">
   Linear Algebra Refresher
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03-Probability-and-Statistics-Refresher.html">
   Probability and Statistics Refresher
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Clustering
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Distances and Timeseries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06-Clustering-I-kmeans.html">
   Clustering I:
   <span class="math notranslate nohighlight">
    \(k\)
   </span>
   -means
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="07-Clustering-II-in-practice.html">
   Clustering II: In Practice
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08-Clustering-III-hierarchical.html">
   Clustering III: Hierarchical Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="09-Clustering-IV-GMM-EM.html">
   Clustering with Mixtures of Gaussians
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="13-Learning-From-Data.html">
   Introduction to Learning From Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="14-Classification-I-Decision-Trees.html">
   Decision Trees
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/05-Distances-Timeseries.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/mcrovella/CS506-Computational-Tools-for-Data-Science"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/mcrovella/CS506-Computational-Tools-for-Data-Science/master?urlpath=tree/05-Distances-Timeseries.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-space-representation">
   Feature space representation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity">
   Similarity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#matrix-representation">
   Matrix representation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#norms">
   Norms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#useful-norms">
   Useful Norms
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-and-dissimilarity">
   Similarity and Dissimilarity
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bit-vectors-and-sets">
   Bit vectors and Sets
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#time-series">
   Time Series
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#similarity-of-time-series">
   Similarity of Time Series
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#norm-based-similarity-measures">
   Norm-based Similarity Measures
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#feature-engineering">
   Feature Engineering
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dynamic-time-warping">
   Dynamic Time Warping
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-timeseries-to-strings">
   From Timeseries to Strings
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="cell tag_hide-cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">config</span> InlineBackend.figure_format=&#39;retina&#39;
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">display_html</span><span class="p">,</span> <span class="n">display</span><span class="p">,</span> <span class="n">Math</span><span class="p">,</span> <span class="n">HTML</span><span class="p">;</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="distances-and-timeseries">
<h1>Distances and Timeseries<a class="headerlink" href="#distances-and-timeseries" title="Permalink to this headline">¶</a></h1>
<p>Today we will start building some tools for making comparisons of data objects, with particular attention to timeseries.</p>
<p>Working with data, we can encounter a wide variety of different data objects:</p>
<ul class="simple">
<li><p>Records of users</p></li>
<li><p>Graphs</p></li>
<li><p>Images</p></li>
<li><p>Videos</p></li>
<li><p>Text (webpages, books)</p></li>
<li><p>Strings (DNA sequences)</p></li>
<li><p>Timeseries</p></li>
<li><p>…</p></li>
</ul>
<p>How can we compare them?</p>
<div class="section" id="feature-space-representation">
<h2>Feature space representation<a class="headerlink" href="#feature-space-representation" title="Permalink to this headline">¶</a></h2>
<p>Usually a data object consists of a set of attributes.</p>
<p>These are also commonly called <strong>features.</strong></p>
<ul class="simple">
<li><p>(“J. Smith”, 25, $ 200,000)</p></li>
<li><p>(“M. Jones”, 47, $ 45,000)</p></li>
</ul>
<p>If all <span class="math notranslate nohighlight">\(d\)</span> dimensions are real-valued then we can visualize each data object as a point in a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector space.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">(25,</span> <span class="pre">USD</span> <span class="pre">200000)</span></code> <span class="math notranslate nohighlight">\(\rightarrow \begin{bmatrix}25\\200000\end{bmatrix}\)</span>.</p></li>
</ul>
<p>Likewise If all features are binary then we can think of each data object as a binary vector in vector space.</p>
<p>The space is called <strong>feature space.</strong></p>
<p>Vector spaces are such a useful tool that we often use them even for non-numeric data.</p>
<p>For example, consider a categorical variable that can be only one of “house”, “tree”, “moon”.</p>
<p>For such a variable, we can use a <strong>one-hot</strong> encoding.</p>
<p>We would encode as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">house</span></code>: <span class="math notranslate nohighlight">\([1, 0, 0]\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tree</span></code>:  <span class="math notranslate nohighlight">\([0, 1, 0]\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">moon</span></code>:  <span class="math notranslate nohighlight">\([0, 0, 1]\)</span></p></li>
</ul>
<p>So an encoding of <code class="docutils literal notranslate"><span class="pre">(25,</span> <span class="pre">USD</span> <span class="pre">200000,</span> <span class="pre">'house')</span></code></p>
<p>could be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}25\\200000\\1\\0\\0\end{bmatrix}.\end{split}\]</div>
<p>We will see many other encodings that take non-numeric data and encode into vectors or matrices.</p>
<p>For example, there are vector or matrix encodings for:</p>
<ul class="simple">
<li><p>Graphs</p></li>
<li><p>Images</p></li>
<li><p>Text</p></li>
</ul>
<p>etc.</p>
</div>
<div class="section" id="similarity">
<h2>Similarity<a class="headerlink" href="#similarity" title="Permalink to this headline">¶</a></h2>
<p>We then are naturally interested in how <strong>similar</strong> or <strong>dissimilar</strong> two objects are.</p>
<p>A dissimilarity function takes two objects as input, and returns a large value when the two objects are not very similar.</p>
<p>Often we put restrictions on the dissimilarity function.</p>
<p>One of the most common is that it be a <strong>metric.</strong></p>
<p>The dissimilarity <span class="math notranslate nohighlight">\(d(x, y)\)</span> between two objects <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> is a
<strong>metric</strong> if</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(d(i, j) = 0 \leftrightarrow i == j\;\;\;\;\;\;\;\;\)</span> (identity of indiscernables)</p></li>
<li><p><span class="math notranslate nohighlight">\(d(i, j) = d(j, i)\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\)</span> (symmetry)</p></li>
<li><p><span class="math notranslate nohighlight">\(d(i, j) \leq d(i, h)+d(h, j)\;\;\)</span> (triangle inequality)</p></li>
</ul>
<center>
<a class="reference internal image-reference" href="_images/TriangleInequality.png"><img alt="Figure" src="_images/TriangleInequality.png" style="width: 350px;" /></a>
</center><p>By WhiteTimberwolf, Brews ohare (PNG version) - PNG version, CC BY-SA 3.0, <a class="reference external" href="https://commons.wikimedia.org/w/index.php?curid=26047092">https://commons.wikimedia.org/w/index.php?curid=26047092</a></p>
<p>A metric is also commonly called a <strong>distance</strong>.</p>
<p>Sometimes we will use “distance” informally, ie, to refer to a dissimilarity function even if we are not sure it is a metric.</p>
<p>We’ll try to say “dissimilarity” in those cases though.</p>
<p>Why is it important or valuable for a dissimilarity to be a metric?</p>
<p>The additional constraints allow us to reason about and more easily visualize the data.</p>
<p>The main way this happens is through the triangle inequality.</p>
<p>The triangle inequality basically says, if two objects are “close” to another object, then they are “close” to each other.</p>
<p>This is not always the case for real data, but when it is true, it can really help.</p>
<p>Definitions of distance or dissimilarity functions are usually
diferent for real, boolean, categorical, and ordinal
variables.</p>
<p>Weights may be associated with diferent variables
based on applications and data semantics.</p>
</div>
<div class="section" id="matrix-representation">
<h2>Matrix representation<a class="headerlink" href="#matrix-representation" title="Permalink to this headline">¶</a></h2>
<p>Very often we will manage data conveniently in matrix form.</p>
<p>The standard way of doing this is:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c}x_{11}\\\vdots\\x_{i1}\\\vdots\\x_{m1}\end{array}&amp;
\begin{array}{c}\dots\\\ddots\\\dots\\\ddots\\\dots\end{array}&amp;
\begin{array}{c}x_{1j}\\\vdots\\x_{ij}\\\vdots\\x_{mj}\end{array}&amp;
\begin{array}{c}\dots\\\ddots\\\dots\\\ddots\\\dots\end{array}&amp;
\begin{array}{c}x_{1n}\\\vdots\\x_{in}\\\vdots\\x_{mn}\end{array}
\end{array}\right]}^{\mbox{$n$ features}} \end{split}\]</div>
<p>Where we typically use symbols <span class="math notranslate nohighlight">\(m\)</span> for number of rows (objects) and <span class="math notranslate nohighlight">\(n\)</span> for number of columns (features).</p>
<p>When we are working with distances, the matrix representation is:</p>
<div class="math notranslate nohighlight">
\[\begin{split} \mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\end{array}\right.\;\;
\overbrace{\left[\begin{array}{cccc}
\begin{array}{c}0\\d(1,2)\\d(1,3)\\\vdots\\d(1,m)\end{array} &amp;
\begin{array}{c}\;\\0\\d(2,3)\\\vdots\\d(2,m)\end{array} &amp;
\begin{array}{c}\;\\\;\\\ddots\\\vdots\\\dots\end{array} &amp;
\begin{array}{c}\;\\\;\\\;\\\;\\0\end{array} \\
\end{array}\right]}^{\mbox{$m$ data objects}} \end{split}\]</div>
</div>
<div class="section" id="norms">
<h2>Norms<a class="headerlink" href="#norms" title="Permalink to this headline">¶</a></h2>
<p>Assume some function <span class="math notranslate nohighlight">\(p(\mathbf{v})\)</span> which measures the “size” of the vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(p()\)</span> is called a <strong>norm</strong> if:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(a\mathbf{v}) = |a|\; p(\mathbf{v})\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\)</span> (absolute scaling)</p></li>
<li><p><span class="math notranslate nohighlight">\(p(\mathbf{u} + \mathbf{v}) \leq p(\mathbf{u}) + p(\mathbf{v})\;\;\;\;\;\;\;\;\;\;\)</span>  (subadditivity)</p></li>
<li><p><span class="math notranslate nohighlight">\(p(\mathbf{v}) = 0 \leftrightarrow \mathbf{v}\)</span> is the zero vector <span class="math notranslate nohighlight">\(\;\;\;\;\)</span>(separates points)</p></li>
</ul>
<p>Norms are important for this reason, among others:</p>
<p><strong>Every norm defines a corresponding metric.</strong></p>
<p>That is, if <span class="math notranslate nohighlight">\(p()\)</span> is a norm, then <span class="math notranslate nohighlight">\(d(\mathbf{x}, \mathbf{y}) = p(\mathbf{x}-\mathbf{y})\)</span> is a metric.</p>
</div>
<div class="section" id="useful-norms">
<h2>Useful Norms<a class="headerlink" href="#useful-norms" title="Permalink to this headline">¶</a></h2>
<p>A general class of norms are called <strong><span class="math notranslate nohighlight">\(\ell_p\)</span></strong> norms, where <span class="math notranslate nohighlight">\(p \geq 1.\)</span></p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} \Vert_p = \left(\sum_{i=1}^d |x_i|^p\right)^{\frac{1}{p}} \]</div>
<p>Notice that we use this notation for the <span class="math notranslate nohighlight">\(p\)</span>-norm of a vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} \Vert_p \]</div>
<p>The corresponding distance that an <span class="math notranslate nohighlight">\(\ell_p\)</span> norm defines is called the <em>Minkowski distance.</em></p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} - \mathbf{y} \Vert_p = \left(\sum_{i=1}^d |x_i - y_i|^p\right)^{\frac{1}{p}} \]</div>
<p>A special – but very important – case is the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm.</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} \Vert_2 = \sqrt{\sum_{i=1}^d |x_i|^2} \]</div>
<p>We’ve already mentioned it: it is the <strong>Euclidean</strong> norm.</p>
<p>The distance defined by the <span class="math notranslate nohighlight">\(\ell_2\)</span> norm is the same as the Euclidean distance between the vectors.</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} - \mathbf{y} \Vert_2  = \sqrt{\sum_{i=1}^d (x_i - y_i)^2} \]</div>
<p>Another important special case is the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm.</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} \Vert_1 = \sum_{i=1}^d |x_i| \]</div>
<p>This defines the <strong>Manhattan</strong> distance, or (for binary vectors), the <strong>Hamming</strong> distance:</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{x} - \mathbf{y} \Vert_1 = \sum_{i=1} |x_i - y_i| \]</div>
<center>
<a class="reference internal image-reference" href="_images/L05-manhattan-distance.png"><img alt="Figure" src="_images/L05-manhattan-distance.png" style="width: 350px;" /></a>
</center><p>If we take the limit of the <span class="math notranslate nohighlight">\(\ell_p\)</span> norm as <span class="math notranslate nohighlight">\(p\)</span> gets large we get the <span class="math notranslate nohighlight">\(\ell_\infty\)</span> norm.</p>
<p>The value of the <span class="math notranslate nohighlight">\(\ell_\infty\)</span> norm is simply the <strong>largest element</strong> in a vector.</p>
<p>What is the metric that this norm induces?</p>
<p>Another related idea is the <span class="math notranslate nohighlight">\(\ell_0\)</span> “norm,” which is not a norm, but is in a sense what we get from the <span class="math notranslate nohighlight">\(p\)</span>-norm for <span class="math notranslate nohighlight">\(p = 0\)</span>.</p>
<p>Note that this is <strong>not</strong> a norm, but it gets called that anyway.</p>
<p>This “norm” simply counts the number of <strong>nonzero</strong> elements in a vector.</p>
<p>This is called the vector’s <strong>sparsity.</strong></p>
<p>Here is the notion of a “circle” under each of three norms.</p>
<p>That is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.</p>
<center>
<a class="reference internal image-reference" href="_images/L5-Vector-Norms.png"><img alt="Figure" src="_images/L5-Vector-Norms.png" style="width: 150px;" /></a>
</center><p><a href="http://creativecommons.org/licenses/by-sa/3.0/" title="Creative Commons Attribution-Share Alike 3.0">CC BY-SA 3.0</a>, <a href="https://commons.wikimedia.org/w/index.php?curid=678101">https://commons.wikimedia.org/w/index.php?curid=678101</a></p>
</div>
<div class="section" id="similarity-and-dissimilarity">
<h2>Similarity and Dissimilarity<a class="headerlink" href="#similarity-and-dissimilarity" title="Permalink to this headline">¶</a></h2>
<p>We’ve already seen that the inner product of two vectors can be used to compute the <strong>cosine of the angle</strong> between them:</p>
<div class="math notranslate nohighlight">
\[ \cos(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x}^T\mathbf{y}}{\Vert\mathbf{x}\Vert \Vert\mathbf{y}\Vert} \]</div>
<p>Note that this value is <strong>large</strong> when <span class="math notranslate nohighlight">\(\mathbf{x} \approx \mathbf{y}.\)</span>  So it is a <strong>similarity</strong> function.</p>
<p>We often find that we have a similarity function <span class="math notranslate nohighlight">\(s\)</span> and need to convert it to a dissimilarity function <span class="math notranslate nohighlight">\(d\)</span>.</p>
<p>Two straightforward ways of doing that are:</p>
<div class="math notranslate nohighlight">
\[d(x,y) = 1\,/\,s(x,y)\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[d(x,y) = k - s(x,y)\]</div>
<p>…for some properly chosen <span class="math notranslate nohighlight">\(k\)</span>.</p>
<p>For cosine similarity, one often uses:</p>
<div class="math notranslate nohighlight">
\[ d(\mathbf{x}, \mathbf{y}) = 1 - \cos(\mathbf{x}, \mathbf{y})\]</div>
<p>Note however that this is <strong>not a metric!</strong></p>
<p>However if we recover the actual angle beween <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, that is a metric.</p>
</div>
<div class="section" id="bit-vectors-and-sets">
<h2>Bit vectors and Sets<a class="headerlink" href="#bit-vectors-and-sets" title="Permalink to this headline">¶</a></h2>
<p>When working with bit vectors, the <span class="math notranslate nohighlight">\(\ell_1\)</span> metric is commonly used and is called the <strong>Hamming</strong> distance.</p>
<center>
<a class="reference internal image-reference" href="_images/L5-hamming-1.png"><img alt="Figure" src="_images/L5-hamming-1.png" style="width: 500px;" /></a>
</center><p>This has a natural interpretation: “how well do the two vectors match?”</p>
<p>Or: “What is the smallest number of bit flips that will convert one vector into the other?”</p>
<center>
<a class="reference internal image-reference" href="_images/L5-hamming-2.png"><img alt="Figure" src="_images/L5-hamming-2.png" style="width: 500px;" /></a>
</center><p>In other cases, the Hamming distance is not a very appropriate metric.</p>
<p>Consider the case in which the bit vector is being used to represent a set.</p>
<p>In that case, Hamming distance measures the <strong>size of the set difference.</strong></p>
<p>For example, consider two documents.   We will use bit vectors to represent the sets of words in each document.</p>
<ul class="simple">
<li><p>Case 1: both documents are large, almost identical, but differ in 10 words.</p></li>
<li><p>Case 2: both documents are small, disjoint, have 5 words each.</p></li>
</ul>
<p>The situation can be represented as this:</p>
<center>
<a class="reference internal image-reference" href="_images/L5-jaccard-1.png"><img alt="Figure" src="_images/L5-jaccard-1.png" style="width: 500px;" /></a>
</center><p>What matters is not just the size of the set difference, but the size of the intersection as well.</p>
<p>This leads to the <em>Jaccard</em> similarity:</p>
<div class="math notranslate nohighlight">
\[J_{Sim}(\mathbf{x}, \mathbf{y}) = \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}\]</div>
<p>This takes on values from 0 to 1, so a natural dissimilarity metric is <span class="math notranslate nohighlight">\(1 - J_{Sim}().\)</span></p>
<p>In fact, this is a <strong>metric!</strong>:</p>
<div class="math notranslate nohighlight">
\[J_{Dist}(\mathbf{x}, \mathbf{y}) = 1- \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}\]</div>
<p>Consider our two cases:</p>
<p>Case 1: (very large almost identical documents)</p>
<center>
<a class="reference internal image-reference" href="_images/L5-jaccard-2.png"><img alt="Figure" src="_images/L5-jaccard-2.png" style="width: 500px;" /></a>
</center><p>Here <span class="math notranslate nohighlight">\(J_{Sim}(\mathbf{x}, \mathbf{y})\)</span> is almost 1.</p>
<p>Case 2: (small disjoint documents)</p>
<center>
<a class="reference internal image-reference" href="_images/L5-jaccard-3.png"><img alt="Figure" src="_images/L5-jaccard-3.png" style="width: 500px;" /></a>
</center><p>Here <span class="math notranslate nohighlight">\(J_{Sim}(\mathbf{x}, \mathbf{y})\)</span> is 0.</p>
</div>
<div class="section" id="time-series">
<h2>Time Series<a class="headerlink" href="#time-series" title="Permalink to this headline">¶</a></h2>
<p>A time series is a sequence of real numbers, representing the measurements of a real variable at (equal) time intervals.</p>
<ul class="simple">
<li><p>Stock prices</p></li>
<li><p>Volume of sales over time</p></li>
<li><p>Daily temperature readings</p></li>
</ul>
<p>A time series database is a large collection of time series.</p>
</div>
<div class="section" id="similarity-of-time-series">
<h2>Similarity of Time Series<a class="headerlink" href="#similarity-of-time-series" title="Permalink to this headline">¶</a></h2>
<p>How should we measure the “similarity” of two timeseries?</p>
<p>We will assume they are the same length.</p>
<p>Examples:</p>
<ul class="simple">
<li><p>Find companies with similar stock price movements over a time interval</p></li>
<li><p>Find similar DNA sequences</p></li>
<li><p>Find users with similar credit usage patterns</p></li>
</ul>
<p>Two Problems:</p>
<ol class="simple">
<li><p>Defining a meaningful similarity (or distance) function.</p></li>
<li><p>Finding an efficient algorithm to compute it.</p></li>
</ol>
</div>
<div class="section" id="norm-based-similarity-measures">
<h2>Norm-based Similarity Measures<a class="headerlink" href="#norm-based-similarity-measures" title="Permalink to this headline">¶</a></h2>
<p>We could just view each sequence as a vector.</p>
<p>Then we could use a <span class="math notranslate nohighlight">\(p\)</span>-norm, eg <span class="math notranslate nohighlight">\(\ell_1, \ell_2,\)</span> or <span class="math notranslate nohighlight">\(\ell_p\)</span> to measure similarity.</p>
<p>Advantages:</p>
<ol class="simple">
<li><p>Easy to compute - linear in the length of the time series (O(n)).</p></li>
<li><p>It is a metric.</p></li>
</ol>
<p>Disadvantage:</p>
<ol class="simple">
<li><p>May not be <strong>meaningful!</strong></p></li>
</ol>
<center>
<a class="reference internal image-reference" href="_images/L5-ts-euclidean.png"><img alt="Figure" src="_images/L5-ts-euclidean.png" style="width: 500px;" /></a>
</center><p>We may believe that <span class="math notranslate nohighlight">\(\mathbf{ts1}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{ts2}\)</span> are the most “similar” pair of timeseries.</p>
<p>However, according to Euclidean distance:</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{ts1} - \mathbf{ts2} \Vert_2 = 26.9,\]</div>
<p>while</p>
<div class="math notranslate nohighlight">
\[ \Vert \mathbf{ts1} - \mathbf{ts3} \Vert_2 = 23.2.\]</div>
<p>Not good!</p>
</div>
<div class="section" id="feature-engineering">
<h2>Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this headline">¶</a></h2>
<p>In general, there may be different aspects of a timeseries that are important in different settings.</p>
<p>The first step therefore is to ask yourself “what is important about timeseries in my application?”</p>
<p>This is an example of <strong>feature engineering.</strong></p>
<p>In other words, feature engineering is the art of computing some derived measure from your data object that makes its important properties usable in a subsequent step.</p>
<p>A reasonable approach may then to be:</p>
<ul class="simple">
<li><p>extract the relevant features</p></li>
<li><p>use a simple method (eg, a norm) to define similarity over those features.</p></li>
</ul>
<p>In the case above, one might think of using</p>
<ul class="simple">
<li><p>Fourier coefficients (to capture periodicity)</p></li>
<li><p>Histograms</p></li>
<li><p>Or something else!</p></li>
</ul>
</div>
<div class="section" id="dynamic-time-warping">
<h2>Dynamic Time Warping<a class="headerlink" href="#dynamic-time-warping" title="Permalink to this headline">¶</a></h2>
<p>One case that arises often is something like the following:  “bump hunting”</p>
<center>
<a class="reference internal image-reference" href="_images/L5-DTW-1.png"><img alt="Figure" src="_images/L5-DTW-1.png" style="width: 500px;" /></a>
</center><p>Both timeseries have the same key characteristics: four bumps.</p>
<p>But a one-one match (ala Euclidean distance) will not detect the similarity.</p>
<p>(Be sure to think about why Euclidean distance will fail here.)</p>
<p>A solution to this is called <strong>dynamic time warping.</strong></p>
<p>The basic idea is to allow acceleration or deceleration of signals along the time dimension.</p>
<p>Classic applications:</p>
<ul class="simple">
<li><p>Speech recognition</p></li>
<li><p>Handwriting recognition</p></li>
</ul>
<p>Specifically:</p>
<ul class="simple">
<li><p>Consider <span class="math notranslate nohighlight">\(X = x_1, x_2, \dots, x_n\)</span> and <span class="math notranslate nohighlight">\(Y = y_1, y_2, \dots, y_n\)</span>.</p></li>
<li><p>We are allowed to extend each sequence by repeating elements to form <span class="math notranslate nohighlight">\(X'\)</span> and <span class="math notranslate nohighlight">\(Y'\)</span>.</p></li>
<li><p>We then calculate, eg, Euclidean distance between the extended sequnces <span class="math notranslate nohighlight">\(X'\)</span> and <span class="math notranslate nohighlight">\(Y'\)</span></p></li>
</ul>
<p>There is a simple way to visualize this algorithm.</p>
<p>Consider a matrix <span class="math notranslate nohighlight">\(M\)</span> where <span class="math notranslate nohighlight">\(M_{ij} = |x_i - y_j|\)</span> (or some other error measure).</p>
<center>
<a class="reference internal image-reference" href="_images/L5-DTW-2.png"><img alt="Figure" src="_images/L5-DTW-2.png" style="width: 500px;" /></a>
</center><p><span class="math notranslate nohighlight">\(M\)</span> measures the amount of error we get if we match <span class="math notranslate nohighlight">\(x_i\)</span> with <span class="math notranslate nohighlight">\(y_j\)</span>.</p>
<p>So we seek a <strong>path through <span class="math notranslate nohighlight">\(M\)</span> that minimizes the total error.</strong></p>
<p>We need to start in the lower left and work our way up via a continuous path.</p>
<p>Basic restrictions on path:</p>
<ul class="simple">
<li><p>Montonicity</p>
<ul>
<li><p>path should not go down or to the left</p></li>
</ul>
</li>
<li><p>Continuity</p>
<ul>
<li><p>No elements may be skipped in sequence</p></li>
</ul>
</li>
</ul>
<p>This can be solved via dynamic programming.  However, the algorithm is still quadratic in <span class="math notranslate nohighlight">\(n\)</span>.</p>
<p>Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.</p>
<p>The basic algorithm looks like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">D</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">M</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">+</span> 
             <span class="nb">min</span><span class="p">(</span> <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span>    <span class="c1"># insertion</span>
                  <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>    <span class="c1"># deletion</span>
                  <span class="n">D</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="p">)</span> <span class="c1"># match</span>
</pre></div>
</div>
<p>Unfortunately, the algorithm is still quadratic in <span class="math notranslate nohighlight">\(n\)</span> – it is <span class="math notranslate nohighlight">\(O(nm)\)</span>.</p>
<p>Hence, we may choose to put a restriction on the amount that the path can deviate from the diagonal.</p>
<p>This is implemented by not allowing the path to pass through locations where <span class="math notranslate nohighlight">\(|i - j| &gt; w\)</span>.</p>
<p>Then the algorithm is <span class="math notranslate nohighlight">\(O(nw)\)</span>.</p>
</div>
<div class="section" id="from-timeseries-to-strings">
<h2>From Timeseries to Strings<a class="headerlink" href="#from-timeseries-to-strings" title="Permalink to this headline">¶</a></h2>
<p>A closely related idea concerns strings.</p>
<p>The key point is that, like timeseries, strings are <strong>sequences</strong>.</p>
<p>Given two strings, one way to define a ‘distance’ between them is:</p>
<ul class="simple">
<li><p>the minimum number of <strong>edit operations</strong> that are needed to transform one string into the other.</p></li>
</ul>
<p>Edit operations are insertion, deletion, and substitution of single characters.</p>
<p>This is called <strong>edit distance</strong> or <strong>Levenshtein distance.</strong></p>
<p>For example, given strings:</p>
<p><code class="docutils literal notranslate"><span class="pre">s</span> <span class="pre">=</span> <span class="pre">VIVALASVEGAS</span></code></p>
<p>and</p>
<p><code class="docutils literal notranslate"><span class="pre">t</span> <span class="pre">=</span> <span class="pre">VIVADAVIS</span></code></p>
<p>we would like to</p>
<ul class="simple">
<li><p>compute the edit distance, and</p></li>
<li><p>obtain the optimal <strong>alignment</strong></p></li>
</ul>
<center>
<a class="reference internal image-reference" href="_images/viva-las-vegas.png"><img alt="Drawing" src="_images/viva-las-vegas.png" style="width: 500px;" /></a>
</center><p>A dynamic programming algorithm can also be used to find this distance,</p>
<p>and it is <strong>very similar to dynamic time-warping.</strong></p>
<p>In bioinformatics this algorithm is called <strong>“Smith-Waterman” sequence alignment.</strong></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="03-Probability-and-Statistics-Refresher.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Probability and Statistics Refresher</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="06-Clustering-I-kmeans.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Clustering I: <span class="math notranslate nohighlight">\(k\)</span>-means</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Mark Crovella<br/>
        
            &copy; Copyright 2021-2022.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>